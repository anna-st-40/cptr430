{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97121933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 1: Agent-Environment Interaction\n",
      "======================================================================\n",
      "\n",
      "Initial state: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "Current percept: (<Location.A: 'A'>, <Status.DIRTY: 'Dirty'>)\n",
      "\n",
      "Executing actions manually:\n",
      "After SUCK: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "After RIGHT: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "After SUCK: [Clean] Agent@B [Clean] | Perf: 19\n",
      "\n",
      "Final Performance: 19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 1: Agents and Environments\n",
    "\"\"\"\n",
    "import random\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "class Location(Enum):\n",
    "    A = \"A\"\n",
    "    B = \"B\"\n",
    "\n",
    "class Status(Enum):\n",
    "    CLEAN = \"Clean\"\n",
    "    DIRTY = \"Dirty\"\n",
    "\n",
    "class Action(Enum):\n",
    "    LEFT = \"Left\"\n",
    "    RIGHT = \"Right\"\n",
    "    SUCK = \"Suck\"\n",
    "    NOOP = \"NoOp\"\n",
    "\n",
    "class VacuumEnvironment:\n",
    "    \"\"\"Simple two-location vacuum environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.locations = {Location.A: Status.DIRTY, Location.B: Status.DIRTY}\n",
    "        self.agent_location = Location.A\n",
    "        self.performance = 0\n",
    "        self.time_steps = 0\n",
    "    \n",
    "    def percept(self) -> Tuple[Location, Status]:\n",
    "        \"\"\"Return current percept: [Location, Status]\"\"\"\n",
    "        return (self.agent_location, self.locations[self.agent_location])\n",
    "    \n",
    "    def execute(self, action: Action):\n",
    "        \"\"\"Execute action and update environment\"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if action == Action.SUCK:\n",
    "            if self.locations[self.agent_location] == Status.DIRTY:\n",
    "                self.locations[self.agent_location] = Status.CLEAN\n",
    "                self.performance += 10  # Reward for cleaning\n",
    "        elif action == Action.LEFT:\n",
    "            self.agent_location = Location.A\n",
    "            self.performance -= 1  # Cost of movement\n",
    "        elif action == Action.RIGHT:\n",
    "            self.agent_location = Location.B\n",
    "            self.performance -= 1  # Cost of movement\n",
    "    \n",
    "    def is_clean(self) -> bool:\n",
    "        return all(status == Status.CLEAN for status in self.locations.values())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.locations[Location.A].value}] Agent@{self.agent_location.value} [{self.locations[Location.B].value}] | Perf: {self.performance}\"\n",
    "\n",
    "# Demonstration\n",
    "print(\"=\"*70)\n",
    "print(\"EXERCISE 1: Agent-Environment Interaction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "env = VacuumEnvironment()\n",
    "print(f\"\\nInitial state: {env}\")\n",
    "print(f\"Current percept: {env.percept()}\")\n",
    "print(\"\\nExecuting actions manually:\")\n",
    "\n",
    "# Manual action sequence\n",
    "env.execute(Action.SUCK)\n",
    "print(f\"After SUCK: {env}\")\n",
    "\n",
    "env.execute(Action.RIGHT)\n",
    "print(f\"After RIGHT: {env}\")\n",
    "\n",
    "env.execute(Action.SUCK)\n",
    "print(f\"After SUCK: {env}\")\n",
    "\n",
    "print(f\"\\nFinal Performance: {env.performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950ba06",
   "metadata": {},
   "source": [
    "1. ## What is the difference between the agent's percept and the full environment state? Consider what information is hidden from the agent.\n",
    "    The full environment has two squares, and the squares have clean/dirty status. The percept only knows the status of the square it is currently on.\n",
    "\n",
    "\n",
    "2. ## How does the performance measure influence what actions are \"good\"? What would/could happen if we changed the reward/cost values?\n",
    "    If it cleans, it is rewarded. If it moves, it has a slight punishment. If the cost of moving was raised, especially if it was greater than the reward of cleaning, the agent might never move at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67153195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXERCISE 2: Simple Reflex Agent Behavior\n",
      "======================================================================\n",
      "\n",
      "Agent Rules:\n",
      "  - IF dirty THEN suck\n",
      "  - IF at A and clean THEN move right\n",
      "  - IF at B and clean THEN move left\n",
      "\n",
      "Step 0: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "  Percept: (<Location.A: 'A'>, <Status.DIRTY: 'Dirty'>) → Action: Suck\n",
      "Step 1: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "  Percept: (<Location.A: 'A'>, <Status.CLEAN: 'Clean'>) → Action: Right\n",
      "Step 2: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Percept: (<Location.B: 'B'>, <Status.DIRTY: 'Dirty'>) → Action: Suck\n",
      "\n",
      "Step 3: [Clean] Agent@B [Clean] | Perf: 19\n",
      "✓ All locations clean!\n",
      "\n",
      "Final Performance Score: 19\n",
      "Steps taken: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 2: Simple Reflex Agent (Rational Behavior)\n",
    "\"\"\"\n",
    "\n",
    "def simple_reflex_vacuum_agent(percept: Tuple[Location, Status]) -> Action:\n",
    "    \"\"\"\n",
    "    Agent function: maps current percept to action\n",
    "    Rules:\n",
    "      - If current location is dirty → SUCK\n",
    "      - If at location A and clean → move RIGHT\n",
    "      - If at location B and clean → move LEFT\n",
    "    \"\"\"\n",
    "    location, status = percept\n",
    "    \n",
    "    if status == Status.DIRTY:\n",
    "        return Action.SUCK\n",
    "    elif location == Location.A:\n",
    "        return Action.RIGHT\n",
    "    else:\n",
    "        return Action.LEFT\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXERCISE 2: Simple Reflex Agent Behavior\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAgent Rules:\")\n",
    "print(\"  - IF dirty THEN suck\")\n",
    "print(\"  - IF at A and clean THEN move right\")\n",
    "print(\"  - IF at B and clean THEN move left\")\n",
    "print()\n",
    "\n",
    "env = VacuumEnvironment()\n",
    "for step in range(8):\n",
    "    percept = env.percept()\n",
    "    action = simple_reflex_vacuum_agent(percept)\n",
    "    print(f\"Step {step}: {env}\")\n",
    "    print(f\"  Percept: {percept} → Action: {action.value}\")\n",
    "    env.execute(action)\n",
    "    if env.is_clean():\n",
    "        print(f\"\\nStep {step+1}: {env}\")\n",
    "        print(\"✓ All locations clean!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal Performance Score: {env.performance}\")\n",
    "print(f\"Steps taken: {env.time_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60cc34",
   "metadata": {},
   "source": [
    "1. ## In this environment, does the agent need memory to act rationally? Why or why not?\n",
    "    No. It does not need to know if the previous square was clean or dirty, it only needs to know if the current one is clean or dirty.\n",
    "\n",
    "2. ## Is this agent rational? Does it maximize expected performance given its percept sequence?\n",
    "    Yes, it is rational, because its percept sequence is to keep trying to clean both squares until they are clean.\n",
    "    \n",
    "3. ## What problem would this agent encounter in a larger environment (e.g., 10 locations)? Think about its rule structure.\n",
    "    It wouldn't know which location isn't clean until it checks all locations, expending cost of moving.\n",
    "\n",
    "4. ## Could the simple_reflex_vacuum_agent get stuck in an infinite loop? Under what circumstances?\n",
    "    Yes, if the locations kept getting dirty while the agent isn't there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd93afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXERCISE 3: Stochastic Environment\n",
      "======================================================================\n",
      "\n",
      "Environment: SUCK action has 70% success rate\n",
      "\n",
      "Step 0: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "  Action: Suck\n",
      "Step 1: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "  Action: Right\n",
      "Step 2: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Action: Suck\n",
      "    ⚠ SUCK action failed!\n",
      "Step 3: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Action: Suck\n",
      "    ⚠ SUCK action failed!\n",
      "Step 4: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Action: Suck\n",
      "    ⚠ SUCK action failed!\n",
      "Step 5: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Action: Suck\n",
      "\n",
      "✓ All locations clean!\n",
      "\n",
      "Final Performance: 19\n",
      "Total steps: 6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 3: Environment Properties - Stochasticity\n",
    "\"\"\"\n",
    "\n",
    "class StochasticVacuumEnvironment(VacuumEnvironment):\n",
    "    \"\"\"Vacuum environment where SUCK action may fail\"\"\"\n",
    "    def execute(self, action: Action):\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if action == Action.SUCK:\n",
    "            if self.locations[self.agent_location] == Status.DIRTY:\n",
    "                # Only 70% success rate\n",
    "                if random.random() > 0.3:\n",
    "                    self.locations[self.agent_location] = Status.CLEAN\n",
    "                    self.performance += 10\n",
    "                else:\n",
    "                    print(\"    ⚠ SUCK action failed!\")\n",
    "        elif action == Action.LEFT:\n",
    "            self.agent_location = Location.A\n",
    "            self.performance -= 1\n",
    "        elif action == Action.RIGHT:\n",
    "            self.agent_location = Location.B\n",
    "            self.performance -= 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXERCISE 3: Stochastic Environment\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nEnvironment: SUCK action has 70% success rate\")\n",
    "print()\n",
    "\n",
    "random.seed(42)  # For reproducible results\n",
    "env_stochastic = StochasticVacuumEnvironment()\n",
    "\n",
    "for step in range(15):\n",
    "    percept = env_stochastic.percept()\n",
    "    action = simple_reflex_vacuum_agent(percept)\n",
    "    print(f\"Step {step}: {env_stochastic}\")\n",
    "    print(f\"  Action: {action.value}\")\n",
    "    env_stochastic.execute(action)\n",
    "    if env_stochastic.is_clean():\n",
    "        print(f\"\\n✓ All locations clean!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal Performance: {env_stochastic.performance}\")\n",
    "print(f\"Total steps: {env_stochastic.time_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e375f43",
   "metadata": {},
   "source": [
    "1. ## Does the simple reflex agent behave rationally in the stochastic environment? Why or why not?\n",
    "    Yes. The only difference here is that the suck action sometimes fails, and the agent just keeps retrying the suck action until it works.\n",
    "2. ## What additional capability would help the agent handle stochasticity better? Think about typical failure logic.\n",
    "    A percept to know why it failed, that would determine whether to retry right away or go do something else and come back later\n",
    "3. ## Compare the performance scores: How much worse is performance in the stochastic environment?\n",
    "    The performance scores are the same because there is no penalty for retrying, only for moving; and the agent does not move until the suck action succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59f3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXERCISE 4: Model-Based Reflex Agent\n",
      "======================================================================\n",
      "\n",
      "Agent maintains internal model of both locations\n",
      "\n",
      "Step 0: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "  Agent's Model: A=Dirty, B=Dirty\n",
      "  Action: Suck\n",
      "\n",
      "Step 1: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "  Agent's Model: A=Clean, B=Dirty\n",
      "  Action: Right\n",
      "\n",
      "Step 2: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Agent's Model: A=Clean, B=Dirty\n",
      "  Action: Suck\n",
      "\n",
      "Step 3: [Clean] Agent@B [Clean] | Perf: 19\n",
      "  Agent's Model: A=Clean, B=Clean\n",
      "  Action: NoOp\n",
      "\n",
      "✓ Agent believes task is complete\n",
      "Final Performance: 19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 4: Model-Based Reflex Agent\n",
    "\"\"\"\n",
    "\n",
    "class ModelBasedVacuumAgent:\n",
    "    \"\"\"Agent that maintains internal state about the world\"\"\"\n",
    "    def __init__(self):\n",
    "        # Internal model of the world state\n",
    "        self.model = {Location.A: Status.DIRTY, Location.B: Status.DIRTY}\n",
    "        self.location = Location.A\n",
    "    \n",
    "    def agent_program(self, percept: Tuple[Location, Status]) -> Action:\n",
    "        location, status = percept\n",
    "        \n",
    "        # Update internal model based on percept\n",
    "        self.location = location\n",
    "        self.model[location] = status\n",
    "        \n",
    "        # Decide action based on model\n",
    "        if status == Status.DIRTY:\n",
    "            return Action.SUCK\n",
    "        elif self.model[Location.A] == Status.DIRTY:\n",
    "            return Action.LEFT\n",
    "        elif self.model[Location.B] == Status.DIRTY:\n",
    "            return Action.RIGHT\n",
    "        else:\n",
    "            return Action.NOOP  # Believes all locations clean\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXERCISE 4: Model-Based Reflex Agent\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAgent maintains internal model of both locations\")\n",
    "print()\n",
    "\n",
    "env = VacuumEnvironment()\n",
    "agent = ModelBasedVacuumAgent()\n",
    "\n",
    "for step in range(10):\n",
    "    percept = env.percept()\n",
    "    action = agent.agent_program(percept)\n",
    "    print(f\"Step {step}: {env}\")\n",
    "    print(f\"  Agent's Model: A={agent.model[Location.A].value}, B={agent.model[Location.B].value}\")\n",
    "    print(f\"  Action: {action.value}\")\n",
    "    env.execute(action)\n",
    "    print()\n",
    "    if action == Action.NOOP:\n",
    "        print(\"✓ Agent believes task is complete\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Performance: {env.performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a03cb6",
   "metadata": {},
   "source": [
    "1. ## How is the model-based agent different from the simple reflex agent? What additional capability does it have?\n",
    "    It keeps an internal memory of what it thinks the world looks like\n",
    "2. ## Could this agent handle a partially observable environment (will it get stuck infinitely)? Why?\n",
    "    It assumes that once a location is clean, it will not get dirty again. It also assumes all locations it hasn't seen are dirty until proven otherwise. So, it shouldn't get stuck - it should just clean all and then stop.\n",
    "3. ## What would happen if the environment changed while the agent wasn't looking (e.g., location A gets dirty again)? Would the agent notice?\n",
    "    No, it would not notice. And, if it was a location that was previously cleaned, the agent would continue assuming it was clean, and would assume all locations are clean at the end if it never visited that location again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462c65b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXERCISE 5: Goal-Based Agent\n",
      "======================================================================\n",
      "\n",
      "Goal: Visit and clean all locations\n",
      "\n",
      "Step 0: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "  Visited: ['A']\n",
      "  Cleaned: []\n",
      "  Action: Suck\n",
      "\n",
      "Step 1: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "  Visited: ['A']\n",
      "  Cleaned: ['A']\n",
      "  Action: Left\n",
      "\n",
      "Step 2: [Clean] Agent@A [Dirty] | Perf: 9\n",
      "  Visited: ['A']\n",
      "  Cleaned: ['A']\n",
      "  Action: Right\n",
      "\n",
      "Step 3: [Clean] Agent@B [Dirty] | Perf: 8\n",
      "  Visited: ['A', 'B']\n",
      "  Cleaned: ['A']\n",
      "  Action: Suck\n",
      "\n",
      "Step 4: [Clean] Agent@B [Clean] | Perf: 18\n",
      "  Visited: ['A', 'B']\n",
      "  Cleaned: ['A', 'B']\n",
      "  Action: NoOp\n",
      "\n",
      "✓ Goal achieved!\n",
      "Final Performance: 18\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 5: Goal-Based Agent\n",
    "\"\"\"\n",
    "\n",
    "class GoalBasedVacuumAgent:\n",
    "    \"\"\"Agent that plans actions to achieve explicit goals\"\"\"\n",
    "    def __init__(self):\n",
    "        self.goal = \"all locations visited and clean\"\n",
    "        self.visited = set()\n",
    "        self.cleaned = set()\n",
    "    \n",
    "    def agent_program(self, percept: Tuple[Location, Status]) -> Action:\n",
    "        location, status = percept\n",
    "        self.visited.add(location)\n",
    "        \n",
    "        # Subgoal: Clean current location if dirty\n",
    "        if status == Status.DIRTY:\n",
    "            return Action.SUCK\n",
    "        else:\n",
    "            self.cleaned.add(location)\n",
    "        \n",
    "        # Plan: Visit unvisited locations\n",
    "        if Location.A not in self.visited:\n",
    "            return random.choice([Action.LEFT, Action.RIGHT])\n",
    "        elif Location.B not in self.visited:\n",
    "            return random.choice([Action.LEFT, Action.RIGHT])\n",
    "        else:\n",
    "            # Goal achieved: all locations visited and clean\n",
    "            return Action.NOOP\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXERCISE 5: Goal-Based Agent\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGoal: Visit and clean all locations\")\n",
    "print()\n",
    "\n",
    "env = VacuumEnvironment()\n",
    "agent = GoalBasedVacuumAgent()\n",
    "\n",
    "for step in range(10):\n",
    "    percept = env.percept()\n",
    "    action = agent.agent_program(percept)\n",
    "    print(f\"Step {step}: {env}\")\n",
    "    print(f\"  Visited: {sorted([loc.value for loc in agent.visited])}\")\n",
    "    print(f\"  Cleaned: {sorted([loc.value for loc in agent.cleaned])}\")\n",
    "    print(f\"  Action: {action.value}\")\n",
    "    env.execute(action)\n",
    "    print()\n",
    "    if action == Action.NOOP:\n",
    "        print(\"✓ Goal achieved!\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Performance: {env.performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e89d7",
   "metadata": {},
   "source": [
    "1. ## How does having an explicit goal make the agent more flexible? Consider alternative scenarios (e.g., blocked paths).\n",
    "    It can reach everywhere without a predefined path. Because there is not just one path, it can take a detour if a path is blocked.\n",
    "2. ## What's the difference between the goal and the plan? Why is this separation useful? Consider a different grid, for example.\n",
    "    The goal can stay the same with different configurations of locations, while the plan will change to suit them.\n",
    "3. ## Could this agent adapt if the goal changed mid-execution (e.g., \"now clean location A twice\")? What would need to change?\n",
    "    It could, if the memory capability was also updated (so that the agent would be able to keep track of how many times the location was cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2305048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXERCISE 6: Utility-Based Agent\n",
      "======================================================================\n",
      "\n",
      "Utility Function: +10 per clean location, -1 per move\n",
      "\n",
      "Step 0: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "  Current state utility: 0\n",
      "  Chosen action: Suck\n",
      "\n",
      "Step 1: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "  Current state utility: 0\n",
      "  Chosen action: Right\n",
      "\n",
      "Step 2: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "  Current state utility: 10\n",
      "  Chosen action: Suck\n",
      "\n",
      "✓ All locations clean!\n",
      "Final Performance: 19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 6: Utility-Based Agent\n",
    "\"\"\"\n",
    "\n",
    "class UtilityBasedVacuumAgent:\n",
    "    \"\"\"Agent that maximizes expected utility\"\"\"\n",
    "    def __init__(self):\n",
    "        self.state = {Location.A: Status.DIRTY, Location.B: Status.DIRTY}\n",
    "        self.location = Location.A\n",
    "    \n",
    "    def utility(self, state: dict) -> float:\n",
    "        \"\"\"Calculate utility of a state\"\"\"\n",
    "        clean_count = sum(1 for s in state.values() if s == Status.CLEAN)\n",
    "        return clean_count * 10  # 10 points per clean location\n",
    "    \n",
    "    def expected_utility(self, action: Action) -> float:\n",
    "        \"\"\"Predict utility after taking action (minus costs)\"\"\"\n",
    "        next_state = self.state.copy()\n",
    "        next_location = self.location\n",
    "        \n",
    "        if action == Action.SUCK:\n",
    "            if self.state[self.location] == Status.DIRTY:\n",
    "                next_state[self.location] = Status.CLEAN\n",
    "            return self.utility(next_state) - 0  # No cost\n",
    "        elif action == Action.LEFT:\n",
    "            next_location = Location.A\n",
    "            # Expected utility: might find dirt there\n",
    "            if next_state[Location.A] == Status.DIRTY:\n",
    "                # If we know it's dirty, we can clean it (net +9)\n",
    "                return self.utility(next_state) + 10 - 1\n",
    "            return self.utility(next_state) - 1  # Just movement cost\n",
    "        elif action == Action.RIGHT:\n",
    "            next_location = Location.B\n",
    "            # Expected utility: might find dirt there\n",
    "            if next_state[Location.B] == Status.DIRTY:\n",
    "                # If we know it's dirty, we can clean it (net +9)\n",
    "                return self.utility(next_state) + 10 - 1\n",
    "            return self.utility(next_state) - 1  # Just movement cost\n",
    "        return self.utility(next_state)\n",
    "    \n",
    "    def agent_program(self, percept: Tuple[Location, Status]) -> Action:\n",
    "        location, status = percept\n",
    "        self.location = location\n",
    "        self.state[location] = status\n",
    "        \n",
    "        # Evaluate all possible actions\n",
    "        actions = [Action.SUCK, Action.LEFT, Action.RIGHT]\n",
    "        action_utilities = []\n",
    "        \n",
    "        for a in actions:\n",
    "            eu = self.expected_utility(a)\n",
    "            action_utilities.append((a, eu))\n",
    "        \n",
    "        # Choose action with maximum expected utility\n",
    "        best_action = max(action_utilities, key=lambda x: x[1])\n",
    "        \n",
    "        return best_action[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXERCISE 6: Utility-Based Agent\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUtility Function: +10 per clean location, -1 per move\")\n",
    "print()\n",
    "\n",
    "env = VacuumEnvironment()\n",
    "agent = UtilityBasedVacuumAgent()\n",
    "\n",
    "for step in range(10):\n",
    "    percept = env.percept()\n",
    "    \n",
    "    # Show utility calculation\n",
    "    print(f\"Step {step}: {env}\")\n",
    "    print(f\"  Current state utility: {agent.utility(agent.state)}\")\n",
    "    \n",
    "    # Get action\n",
    "    action = agent.agent_program(percept)\n",
    "    print(f\"  Chosen action: {action.value}\")\n",
    "    \n",
    "    env.execute(action)\n",
    "    print()\n",
    "    if env.is_clean():\n",
    "        print(\"✓ All locations clean!\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Performance: {env.performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd75007",
   "metadata": {},
   "source": [
    "1. ## How does the utility function encode the agent's preferences? What would happen if we changed the values?\n",
    "    The agent prefers to have a clean location, since it gets 10 points per location that is clean. If we increased the penalty for moving beyond the reward for cleaning, it would never move, since it always selects the action with the most utility.\n",
    "2. ## When would a utility-based agent be better than a goal-based agent? Think about scenarios with competing objectives.\n",
    "    Better when the goal is ambigious, such as maximizing something. Or, when there are multiple goals (e.g. drive to this location but also don't kill anyone), it decides which one is more important.\n",
    "3. ## How could we extend the utility function to consider time (e.g., finish faster gets higher utility)?\n",
    "    Give a penalty for all actions, not just the move action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1a8f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXERCISE 7: Agent Performance Comparison\n",
      "======================================================================\n",
      "\n",
      "Running all four agent types in identical environments...\n",
      "\n",
      "Results:\n",
      "----------------------------------------------------------------------\n",
      "Agent Type           Performance     Steps     \n",
      "----------------------------------------------------------------------\n",
      "Simple Reflex        19              3         \n",
      "Model-Based          19              4         \n",
      "Goal-Based           12              10        \n",
      "Utility-Based        19              3         \n",
      "----------------------------------------------------------------------\n",
      "\n",
      " Best Performance: Simple Reflex (Score: 19)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 7: Comparing All Agent Types\n",
    "\"\"\"\n",
    "\n",
    "def run_agent_comparison():\n",
    "    \"\"\"Run all four agent types and compare performance\"\"\"\n",
    "    \n",
    "    def run_simple_reflex(max_steps=10):\n",
    "        env = VacuumEnvironment()\n",
    "        for _ in range(max_steps):\n",
    "            action = simple_reflex_vacuum_agent(env.percept())\n",
    "            env.execute(action)\n",
    "            if env.is_clean():\n",
    "                break\n",
    "        return env.performance, env.time_steps\n",
    "    \n",
    "    def run_model_based(max_steps=10):\n",
    "        env = VacuumEnvironment()\n",
    "        agent = ModelBasedVacuumAgent()\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.agent_program(env.percept())\n",
    "            env.execute(action)\n",
    "            if action == Action.NOOP:\n",
    "                break\n",
    "        return env.performance, env.time_steps\n",
    "    \n",
    "    def run_goal_based(max_steps=10):\n",
    "        env = VacuumEnvironment()\n",
    "        agent = GoalBasedVacuumAgent()\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.agent_program(env.percept())\n",
    "            env.execute(action)\n",
    "            if action == Action.NOOP:\n",
    "                break\n",
    "        return env.performance, env.time_steps\n",
    "    \n",
    "    def run_utility_based(max_steps=10):\n",
    "        env = VacuumEnvironment()\n",
    "        agent = UtilityBasedVacuumAgent()\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.agent_program(env.percept())\n",
    "            env.execute(action)\n",
    "            if env.is_clean():\n",
    "                break\n",
    "        return env.performance, env.time_steps\n",
    "    \n",
    "    return {\n",
    "        \"Simple Reflex\": run_simple_reflex(),\n",
    "        \"Model-Based\": run_model_based(),\n",
    "        \"Goal-Based\": run_goal_based(),\n",
    "        \"Utility-Based\": run_utility_based()\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXERCISE 7: Agent Performance Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRunning all four agent types in identical environments...\\n\")\n",
    "\n",
    "results = run_agent_comparison()\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Agent Type':<20} {'Performance':<15} {'Steps':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for agent_type, (perf, steps) in results.items():\n",
    "    print(f\"{agent_type:<20} {perf:<15} {steps:<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Find best performer\n",
    "best_agent = max(results.items(), key=lambda x: x[1][0])\n",
    "print(f\"\\n Best Performance: {best_agent[0]} (Score: {best_agent[1][0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108760a",
   "metadata": {},
   "source": [
    "1. ## Did all agents achieve the same performance? If not, why do you think they differ?\n",
    "    All except the goal-based agent. This was because the goal-based agent kept visiting different locations randomly, not optimizing performance.\n",
    "2. ## In general, is the highest-performing agent always the \"best\" choice? Consider factors beyond performance score.\n",
    "    Not always. In a more complex scenario, you might not want to hard-code best performance, and instead have the agent make its own decisions.\n",
    "3. ## For this specific environment (small, deterministic, fully observable), is the complexity of utility-based agents justified? When would it be justified?\n",
    "    Here, the simple reflex and the utility-based agents had the same performance in the same amount of steps, while the simple reflex agent was much simpler to code. So, the utility-based agent was not justified in this scenario. In a larger, non-deterministic, not fully observable environment, we would not be able to accomplish the goal with a simple reflex agent, so the utility-based agent would be justified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cptr430 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
