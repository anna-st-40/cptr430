{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97121933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 1: Agent-Environment Interaction\n",
      "======================================================================\n",
      "\n",
      "Initial state: [Dirty] Agent@A [Dirty] | Perf: 0\n",
      "Current percept: (<Location.A: 'A'>, <Status.DIRTY: 'Dirty'>)\n",
      "\n",
      "Executing actions manually:\n",
      "After SUCK: [Clean] Agent@A [Dirty] | Perf: 10\n",
      "After RIGHT: [Clean] Agent@B [Dirty] | Perf: 9\n",
      "After SUCK: [Clean] Agent@B [Clean] | Perf: 19\n",
      "\n",
      "Final Performance: 19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exercise 1: Agents and Environments\n",
    "\"\"\"\n",
    "import random\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "class Location(Enum):\n",
    "    A = \"A\"\n",
    "    B = \"B\"\n",
    "\n",
    "class Status(Enum):\n",
    "    CLEAN = \"Clean\"\n",
    "    DIRTY = \"Dirty\"\n",
    "\n",
    "class Action(Enum):\n",
    "    LEFT = \"Left\"\n",
    "    RIGHT = \"Right\"\n",
    "    SUCK = \"Suck\"\n",
    "    NOOP = \"NoOp\"\n",
    "\n",
    "class VacuumEnvironment:\n",
    "    \"\"\"Simple two-location vacuum environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.locations = {Location.A: Status.DIRTY, Location.B: Status.DIRTY}\n",
    "        self.agent_location = Location.A\n",
    "        self.performance = 0\n",
    "        self.time_steps = 0\n",
    "    \n",
    "    def percept(self) -> Tuple[Location, Status]:\n",
    "        \"\"\"Return current percept: [Location, Status]\"\"\"\n",
    "        return (self.agent_location, self.locations[self.agent_location])\n",
    "    \n",
    "    def execute(self, action: Action):\n",
    "        \"\"\"Execute action and update environment\"\"\"\n",
    "        self.time_steps += 1\n",
    "        \n",
    "        if action == Action.SUCK:\n",
    "            if self.locations[self.agent_location] == Status.DIRTY:\n",
    "                self.locations[self.agent_location] = Status.CLEAN\n",
    "                self.performance += 10  # Reward for cleaning\n",
    "        elif action == Action.LEFT:\n",
    "            self.agent_location = Location.A\n",
    "            self.performance -= 1  # Cost of movement\n",
    "        elif action == Action.RIGHT:\n",
    "            self.agent_location = Location.B\n",
    "            self.performance -= 1  # Cost of movement\n",
    "    \n",
    "    def is_clean(self) -> bool:\n",
    "        return all(status == Status.CLEAN for status in self.locations.values())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.locations[Location.A].value}] Agent@{self.agent_location.value} [{self.locations[Location.B].value}] | Perf: {self.performance}\"\n",
    "\n",
    "# Demonstration\n",
    "print(\"=\"*70)\n",
    "print(\"EXERCISE 1: Agent-Environment Interaction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "env = VacuumEnvironment()\n",
    "print(f\"\\nInitial state: {env}\")\n",
    "print(f\"Current percept: {env.percept()}\")\n",
    "print(\"\\nExecuting actions manually:\")\n",
    "\n",
    "# Manual action sequence\n",
    "env.execute(Action.SUCK)\n",
    "print(f\"After SUCK: {env}\")\n",
    "\n",
    "env.execute(Action.RIGHT)\n",
    "print(f\"After RIGHT: {env}\")\n",
    "\n",
    "env.execute(Action.SUCK)\n",
    "print(f\"After SUCK: {env}\")\n",
    "\n",
    "print(f\"\\nFinal Performance: {env.performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950ba06",
   "metadata": {},
   "source": [
    "1. ## What is the difference between the agent's percept and the full environment state? Consider what information is hidden from the agent.\n",
    "    The full environment has two squares, and the squares have clean/dirty status. The percept only knows the status of the square it is currently on.\n",
    "\n",
    "\n",
    "2. ## How does the performance measure influence what actions are \"good\"? What would/could happen if we changed the reward/cost values?\n",
    "    If it cleans, it is rewarded. If it moves, it has a slight punishment. If the cost of moving was raised, especially if it was greater than the reward of cleaning, the agent might never move at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cptr430 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
